{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-course on Deep Learning With Python\n",
    "=============\n",
    "\n",
    "Lesson 1: Introduction to Theano\n",
    "------------\n",
    "Theano is a Python library for fast numerical computation to aid in the development of deep learning models. At it’s heart Theano is a compiler for mathematical expressions in Python. It knows how to take your structures and turn them into very efficient code that uses NumPy and efficient native libraries to run as fast as possible on CPUs or GPUs.\n",
    "\n",
    "For more detail information, visit [Theano homepage](http://deeplearning.net/software/theano/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small example of a Theano program is listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor\n",
    "# declare two symbolic floating-point scalars\n",
    "a = tensor.dscalar()\n",
    "b = tensor.dscalar()\n",
    "# create a simple expression\n",
    "c = a + b\n",
    "# convert the expression into a callable object that takes (a,b) values as input and compute a value for c\n",
    "f = theano.function([a,b],c)\n",
    "# example: bind 1.5 to a, 2.5 to b and evaluate c\n",
    "result = f(1.5, 2.5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson 2: Introduction to TensorFlow\n",
    "------------\n",
    "TensorFlow is a Python library for fast numerical computing created and released by Google. Like Theano, TensorFlow is intended to be used to develop deep learning models. With the backing of Google, perhaps used in some of it’s production systems and used by the Google DeepMind research group, it is a platform that we cannot ignore. Unlike Theano, TensorFlow does have more of a production focus with a capability to run on CPUs, GPUs and even very large clusters.\n",
    "\n",
    "For more detail information, visit [Tensorflow homepage](https://www.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a small example of a TensorFlow program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# declare two symbolic floating-point scalars\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "# create a simnle symbolic expression using the add function\n",
    "add = tf.add(a,b)\n",
    "# bind 1.5 to 'a', 2.5 to 'b' and evaluate 'c'\n",
    "sess = tf.Session()\n",
    "binding = {a: 1.5, b: 2.5}\n",
    "c = sess.run(add, feed_dict=binding)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson 3: Introduction to Keras\n",
    "------------\n",
    "A difficulty of both Theano and TensorFlow is that it can take a lot of code to create even very simple neural network models. These libraries were designed primarily as a platform for research and development more than for the practical concerns of applied deep learning. The Keras library addresses these concerns by providing a wrapper for both Theano and Keras. It provides a clean and simple API that allows you to define and evaluate deep learning models in just a few lines of code.\n",
    "\n",
    "Because of the ease of use and because it leverages the power of Theano and TensorFlow, Keras is quickly becoming the go-to library for applied deep learning. The focus of Keras is the concept of a model. The life-cycle of a model can be summarized as follows:\n",
    "1. Define your model. Create a Sequential model and add configured layers\n",
    "2. Compile your model. Specify loss function and optimizers and call the `compile()` function on the model.\n",
    "3. Fit your model. Train the model on a sample data by calling the `fit()` funciton on the model\n",
    "4. Make prediction. User the odel to generate predictions on new data by calling functions such as `evaluate()` or `predict()` on the model\n",
    "\n",
    "Visit [Keras hompage](https://keras.io/) for more detail information.\n",
    "\n",
    "Take some time to play with Keras and get familiar with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main type of model in Keras is the Sequential model. Here's is an example of the Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking layers is as easy as `.add()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(output_dim=64, input_dim=100))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=10))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model looks good, configure its laerning process with `complile()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, the model is ready to be fitted with `fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.random.random((1000, 100))\n",
    "Y_train = np.random.randint(2, size=(1000, 10))\n",
    "\n",
    "X_test = np.random.random((20, 100))\n",
    "Y_test = np.random.randint(2, size=(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s - loss: 11.7874     \n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s - loss: 11.6433     \n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s - loss: 11.6152     \n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s - loss: 11.5976     \n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s - loss: 11.5841     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11bce5550>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, nb_epoch=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can evaluate the model's performance in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can generate predictions on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s\n",
      "20/20 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "classes = model.predict_classes(X_test, batch_size=32)\n",
    "proba = model.predict_proba(X_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more in-depth tutorial about Keras, you can check out:\n",
    "- [Getting started with the Sequential model](https://keras.io/getting-started/sequential-model-guide/)\n",
    "- [Getting started with the functional API](https://keras.io/getting-started/functional-api-guide/)\n",
    "\n",
    "In the [examples folder](https://github.com/fchollet/keras/tree/master/examples) of the Github repository, you will find more advanced models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson 4: Crash Course in Multilayer Perceptrons\n",
    "------------\n",
    "Artificial neural networks are a fascinating area of study, although they can be intimidating when just getting started. The field of artificial neural networks is often just called neural networks or Multilayer Perceptrons after perhaps the most useful type of neural network. The building block for neural networks are artificial neurons. These are simple computational units that have weighted input signals and produce an output signal using an activation function.\n",
    "\n",
    "Neurons are arranged into networks of neurons. A row of neurons is called a layer and one network can have multiple layers. The architecture of the neurons in the network is often called the network topology. Once configured, the neural network needs to be trained on your dataset. The classical and still preferred training algorithm for neural networks is called stochastic gradient descent.\n",
    "\n",
    "![Neuron](figs/neuron.png)\n",
    "\n",
    "- Review Andrew Ng's course: [https://www.coursera.org/learn/machine-learning/home/week/4](https://www.coursera.org/learn/machine-learning/home/week/4)\n",
    "\n",
    "- Book reading: [Deep Learning Book](http://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lesson 05: First Neural Network in Keras\n",
    "------------\n",
    "\n",
    "Keras allows you to develop and evaluate deep learning models in very few lines of code. In this lesson the goal is to develop the first neural network using the Keras library. Use a standard binary (two-class) classification dataset from the UCI Machine Learning Repository, like the [Pima Indians](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) or the [ionosphere datasets](https://archive.ics.uci.edu/ml/datasets/Ionosphere). Piece together code to achieve the following:\n",
    "1. Load dataset using NumPy or Pandas\n",
    "2. Define neural network model and compile it\n",
    "3. Fit the model to the dataset\n",
    "4. Estimate the performance of the model on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s - loss: 0.6826 - acc: 0.6328     \n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s - loss: 0.6590 - acc: 0.6510     \n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s - loss: 0.6475 - acc: 0.6549     \n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s - loss: 0.6416 - acc: 0.6615     \n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s - loss: 0.6216 - acc: 0.6745     \n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s - loss: 0.6128 - acc: 0.6680     \n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s - loss: 0.6018 - acc: 0.6927     \n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s - loss: 0.5962 - acc: 0.6927     \n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s - loss: 0.5991 - acc: 0.6953     \n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s - loss: 0.5920 - acc: 0.6927     \n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s - loss: 0.5905 - acc: 0.6979     \n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s - loss: 0.5883 - acc: 0.6901     \n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s - loss: 0.5870 - acc: 0.6953     \n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s - loss: 0.5869 - acc: 0.6836     \n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s - loss: 0.5815 - acc: 0.6953     \n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s - loss: 0.5779 - acc: 0.6966     \n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s - loss: 0.5809 - acc: 0.6849     \n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s - loss: 0.5818 - acc: 0.6953     \n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s - loss: 0.5814 - acc: 0.6901     \n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s - loss: 0.5748 - acc: 0.7096     \n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s - loss: 0.5758 - acc: 0.7005     \n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s - loss: 0.5739 - acc: 0.7135     \n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s - loss: 0.5736 - acc: 0.6927     \n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s - loss: 0.5750 - acc: 0.6940     \n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s - loss: 0.5734 - acc: 0.7031     \n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s - loss: 0.5683 - acc: 0.7083     \n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s - loss: 0.5688 - acc: 0.7018     \n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s - loss: 0.5714 - acc: 0.7070     \n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s - loss: 0.5621 - acc: 0.7188     \n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s - loss: 0.5647 - acc: 0.7122     \n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s - loss: 0.5630 - acc: 0.7135     \n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s - loss: 0.5613 - acc: 0.7214     \n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s - loss: 0.5594 - acc: 0.7188     \n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s - loss: 0.5598 - acc: 0.7187     \n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s - loss: 0.5624 - acc: 0.7187     \n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s - loss: 0.5615 - acc: 0.7201     \n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s - loss: 0.5544 - acc: 0.7214     \n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s - loss: 0.5529 - acc: 0.7135     \n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s - loss: 0.5550 - acc: 0.7227     \n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s - loss: 0.5574 - acc: 0.7331     \n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s - loss: 0.5562 - acc: 0.7357     \n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s - loss: 0.5458 - acc: 0.7370     \n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s - loss: 0.5487 - acc: 0.7253     \n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s - loss: 0.5409 - acc: 0.7344     \n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s - loss: 0.5445 - acc: 0.7435     \n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s - loss: 0.5362 - acc: 0.7357     \n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s - loss: 0.5400 - acc: 0.7357     \n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s - loss: 0.5354 - acc: 0.7409     \n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s - loss: 0.5406 - acc: 0.7357     \n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s - loss: 0.5476 - acc: 0.7357     \n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s - loss: 0.5288 - acc: 0.7461     \n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s - loss: 0.5284 - acc: 0.7474     \n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s - loss: 0.5306 - acc: 0.7370     \n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s - loss: 0.5283 - acc: 0.7487     \n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s - loss: 0.5252 - acc: 0.7539     \n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s - loss: 0.5257 - acc: 0.7552     \n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s - loss: 0.5282 - acc: 0.7422     \n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s - loss: 0.5269 - acc: 0.7513     \n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s - loss: 0.5248 - acc: 0.7487     \n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s - loss: 0.5202 - acc: 0.7500     \n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s - loss: 0.5160 - acc: 0.7552     \n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s - loss: 0.5161 - acc: 0.7461     \n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s - loss: 0.5100 - acc: 0.7591     \n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s - loss: 0.5145 - acc: 0.7526     \n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s - loss: 0.5125 - acc: 0.7474     \n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s - loss: 0.5148 - acc: 0.7617     \n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s - loss: 0.5088 - acc: 0.7539     \n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s - loss: 0.5124 - acc: 0.7721     \n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s - loss: 0.5152 - acc: 0.7487     \n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s - loss: 0.5163 - acc: 0.7552     \n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s - loss: 0.5195 - acc: 0.7422     \n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s - loss: 0.5094 - acc: 0.7461     \n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s - loss: 0.4984 - acc: 0.7617     \n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s - loss: 0.4979 - acc: 0.7617     \n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s - loss: 0.5073 - acc: 0.7591     \n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s - loss: 0.5028 - acc: 0.7513     \n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s - loss: 0.5018 - acc: 0.7552     \n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s - loss: 0.4929 - acc: 0.7578     \n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s - loss: 0.5011 - acc: 0.7630     \n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s - loss: 0.5018 - acc: 0.7474     \n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s - loss: 0.5026 - acc: 0.7578     \n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s - loss: 0.4986 - acc: 0.7604     \n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s - loss: 0.4948 - acc: 0.7578     \n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s - loss: 0.4915 - acc: 0.7747     \n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s - loss: 0.4982 - acc: 0.7578     \n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s - loss: 0.4906 - acc: 0.7721     \n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s - loss: 0.4972 - acc: 0.7578     \n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s - loss: 0.4921 - acc: 0.7695     \n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s - loss: 0.4900 - acc: 0.7630     \n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s - loss: 0.4870 - acc: 0.7669     \n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s - loss: 0.4979 - acc: 0.7591     \n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s - loss: 0.4943 - acc: 0.7682     \n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s - loss: 0.4907 - acc: 0.7630     \n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s - loss: 0.4859 - acc: 0.7617     \n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s - loss: 0.4833 - acc: 0.7695     \n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s - loss: 0.4842 - acc: 0.7695     \n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s - loss: 0.4753 - acc: 0.7786     \n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s - loss: 0.4990 - acc: 0.7422     \n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s - loss: 0.4856 - acc: 0.7695     \n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s - loss: 0.4814 - acc: 0.7721     \n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s - loss: 0.4857 - acc: 0.7695     \n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s - loss: 0.4769 - acc: 0.7878     \n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s - loss: 0.4844 - acc: 0.7552     \n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s - loss: 0.4817 - acc: 0.7760     \n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s - loss: 0.4803 - acc: 0.7760     \n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s - loss: 0.4811 - acc: 0.7812     \n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s - loss: 0.4836 - acc: 0.7773     \n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s - loss: 0.4774 - acc: 0.7826     \n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s - loss: 0.4775 - acc: 0.7786     \n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s - loss: 0.4795 - acc: 0.7695     \n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s - loss: 0.4713 - acc: 0.7747     \n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s - loss: 0.4745 - acc: 0.7826     \n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s - loss: 0.4751 - acc: 0.7773     \n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s - loss: 0.4808 - acc: 0.7617     \n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s - loss: 0.4790 - acc: 0.7643     \n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s - loss: 0.4723 - acc: 0.7786     \n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s - loss: 0.4668 - acc: 0.7799     \n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s - loss: 0.4644 - acc: 0.7839     \n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s - loss: 0.4711 - acc: 0.7839     \n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s - loss: 0.4694 - acc: 0.7839     \n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s - loss: 0.4760 - acc: 0.7839     \n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s - loss: 0.4725 - acc: 0.7747     \n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s - loss: 0.4699 - acc: 0.7826     \n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s - loss: 0.4701 - acc: 0.7799     \n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s - loss: 0.4695 - acc: 0.7747     \n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s - loss: 0.4642 - acc: 0.7799     \n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s - loss: 0.4719 - acc: 0.7656     \n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s - loss: 0.4579 - acc: 0.7865     \n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s - loss: 0.4644 - acc: 0.7865     \n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s - loss: 0.4735 - acc: 0.7760     \n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s - loss: 0.4606 - acc: 0.7773     \n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s - loss: 0.4639 - acc: 0.7760     \n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s - loss: 0.4751 - acc: 0.7865     \n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s - loss: 0.4729 - acc: 0.7773     \n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s - loss: 0.4601 - acc: 0.7917     \n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s - loss: 0.4564 - acc: 0.7852     \n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s - loss: 0.4625 - acc: 0.7852     \n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s - loss: 0.4684 - acc: 0.7760     \n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s - loss: 0.4605 - acc: 0.7734     \n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s - loss: 0.4590 - acc: 0.7839     \n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s - loss: 0.4602 - acc: 0.7734     \n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s - loss: 0.4593 - acc: 0.7760     \n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s - loss: 0.4614 - acc: 0.7878     \n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s - loss: 0.4508 - acc: 0.7969     \n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s - loss: 0.4580 - acc: 0.7747     \n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s - loss: 0.4627 - acc: 0.7812     \n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s - loss: 0.4531 - acc: 0.7943     \n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s - loss: 0.4656 - acc: 0.7734     \n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s - loss: 0.4566 - acc: 0.7839     \n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s - loss: 0.4593 - acc: 0.7839     \n",
      " 32/768 [>.............................] - ETA: 0sacc 79.56%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = numpy.loadtxt(\"data/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# Define and Compile\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, Y, nb_epoch=150, batch_size=10)\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"%s %.2f%%\" %(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lesson 06: Use Keras Models with Scikit-Learn\n",
    "------------\n",
    "\n",
    "The scikit-learn library is a general purpose machine learning framework in Python built on top of SciPy. Scikit-learn excels at tasks such as evaluating model performance and optimizing model hyperparameters in just a few lines of code. Keras provides a wrapper class that allows you to use your deep learning models with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See example in the [Notebook](../kaggle/Allstate Competition.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
